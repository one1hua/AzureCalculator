import os
import traceback
from typing import Dict, Any, Optional, Literal, List

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain_chroma import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langgraph.graph import StateGraph
from langchain_core.chat_history import InMemoryChatMessageHistory

# =========================================================
# FASTAPI
# =========================================================
app = FastAPI(title="RAG (MD + CSV) with KB filter")

# =========================================================
# Azure OpenAI ì„¤ì • (ë³¸ì¸ ê°’ìœ¼ë¡œ êµì²´)
#   - ì—”ë“œí¬ì¸íŠ¸ëŠ” 'ë² ì´ìŠ¤ URL'ë§Œ! (ë’¤ì— /openai/deployments/... ê¸ˆì§€)
#   - ë°°í¬ ì´ë¦„ì€ OpenAI Studio/í¬í„¸ì˜ 'Deployment name'
#   - ë²„ì „ì€ curlë¡œ í™•ì¸ëœ 2023-05-15 ë¡œ ìš°ì„  í†µì¼
# =========================================================
AZURE_API_KEY = "8D0TqUJQKj7SfHCMcRSAs5cJBSbvvQIcsuZ6QvHFKWsVsP9JyhooJQQJ99BGACNns7RXJ3w3AAABACOG0jZk"
AZURE_ENDPOINT_BASE = "https://azure-openai-price01.openai.azure.com"  # ì˜ˆì‹œ
OPENAI_API_VERSION = "2023-05-15"

AZURE_CHAT_DEPLOYMENT = "gpt-4o"                 # ì˜ˆì‹œ ë°°í¬ëª…
AZURE_EMBED_DEPLOYMENT = "text-embedding-3-small"  # ì˜ˆì‹œ ë°°í¬ëª…

# =========================================================
# ëª¨ë¸ ì´ˆê¸°í™”
# =========================================================
azure_llm = AzureChatOpenAI(
    api_key=AZURE_API_KEY,
    azure_endpoint=AZURE_ENDPOINT_BASE,
    azure_deployment=AZURE_CHAT_DEPLOYMENT,
    openai_api_version=OPENAI_API_VERSION,
    temperature=0.2,
    max_tokens=800,
    request_timeout=30,
)

embedding_model = AzureOpenAIEmbeddings(
    api_key=AZURE_API_KEY,
    azure_endpoint=AZURE_ENDPOINT_BASE,
    azure_deployment=AZURE_EMBED_DEPLOYMENT,
    openai_api_version=OPENAI_API_VERSION,
)

# =========================================================
# Chroma ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ ê¶Œì¥)
# =========================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIR = os.path.join(BASE_DIR, "chromaDB")

def make_retriever(kb: Optional[str], k: int = 3):
    """KB ë©”íƒ€ë°ì´í„° í•„í„° ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±"""
    search_kwargs = {"k": k}
    if kb:
        search_kwargs["filter"] = {"kb": kb}
    return Chroma(
        persist_directory=PERSIST_DIR,
        embedding_function=embedding_model
    ).as_retriever(search_kwargs=search_kwargs)

# ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„ (í•„ìš” ì‹œ ìš”ì²­ë§ˆë‹¤ make_retrieverë¡œ ìƒˆë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ)
retriever_md_base = make_retriever("kb_md", k=3)
retriever_csv_base = make_retriever("kb_csv", k=3)
retriever_all_base = make_retriever(None, k=3)  # ì „ì²´(í•„í„° ì—†ìŒ)

# =========================================================
# ì„¸ì…˜ë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬
# =========================================================
chats_by_session_id: Dict[str, InMemoryChatMessageHistory] = {}

def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in chats_by_session_id:
        chats_by_session_id[session_id] = InMemoryChatMessageHistory()
    return chats_by_session_id[session_id]

# =========================================================
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
# =========================================================
class QARequest(BaseModel):
    question: str
    session_id: str
    target_kb: Optional[Literal["md", "csv", "all"]] = "all"
    k: int = 3

# =========================================================
# ìœ í‹¸: ë¬¸ë§¥/ì°¸ì¡° ë³€í™˜
# =========================================================
def docs_to_context(docs) -> str:
    return "\n\n---\n\n".join(d.page_content for d in docs)

def docs_to_refs(docs) -> List[Dict[str, Any]]:
    refs = []
    for d in docs:
        meta = d.metadata or {}
        refs.append({
            "kb": meta.get("kb"),
            "source": meta.get("source"),
            "row_index": meta.get("row_index"),
            "chunk_index": meta.get("chunk_index"),
            "snippet": (d.page_content[:200] + "â€¦") if len(d.page_content) > 200 else d.page_content,
        })
    return refs

def build_prompt(context: str, question: str, history_text: str = "") -> str:
    pre = f"ë‹¤ìŒì€ ë¬¸ë§¥ì…ë‹ˆë‹¤:\n{context}\n\n"
    if history_text:
        pre = f"ë‹¤ìŒì€ ê³¼ê±° ëŒ€í™”ì™€ ë¬¸ë§¥ì…ë‹ˆë‹¤:\n{history_text}\n\n{pre}"
    rules = (
        "ê·œì¹™:\n"
        "- ë°˜ë“œì‹œ ë¬¸ë§¥ì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n"
        "- ë¬¸ë§¥ì— ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê³ , ë‹¤ìŒ íƒìƒ‰ ë°©í–¥ì„ ì œì•ˆí•˜ì„¸ìš”.\n"
        "- ê°€ëŠ¥í•˜ë©´ ì¶œì²˜ íŒŒì¼/í–‰/ì„¹ì…˜ ë“±ì˜ ë‹¨ì„œë¥¼ í•¨ê»˜ ì¨ì£¼ì„¸ìš”.\n\n"
    )
    return f"{pre}{rules}ì§ˆë¬¸: {question}\n\në‹µë³€:"

def select_docs(question: str, target_kb: str, k: int):
    """KB ì„ íƒì— ë”°ë¼ ë¬¸ì„œ ê²€ìƒ‰"""
    if target_kb == "md":
        return make_retriever("kb_md", k=k).invoke(question)
    if target_kb == "csv":
        return make_retriever("kb_csv", k=k).invoke(question)
    # all: ë‘ KBë¥¼ ê°ê° ì¡°íšŒ í›„ í•©ì¹˜ê¸° (ê°„ë‹¨ ë³‘í•©)
    docs_md = make_retriever("kb_md", k=k).invoke(question)
    docs_csv = make_retriever("kb_csv", k=k).invoke(question)
    return docs_md + docs_csv

# =========================================================
# RAG ë…¸ë“œ (LangGraph)
# =========================================================
def rag_node(state: Dict[str, Any]):
    mcp = state["mcp"]
    payload = mcp["payload"]

    question: str = payload["question"]
    meta: Dict[str, Any] = payload.get("metadata", {})

    session_id: str = meta.get("session_id", "default")
    target_kb: str = meta.get("target_kb", "all")
    top_k: int = int(meta.get("k", 3))

    chat_history = get_chat_history(session_id)

    print(f"ğŸ” ì§ˆë¬¸: {question} | target_kb={target_kb} | k={top_k}")
    docs = select_docs(question, target_kb, top_k)
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    # ë¹ˆ ê²€ìƒ‰ì€ 200ìœ¼ë¡œ ì•ˆë‚´ ë°˜í™˜
    if not docs:
        result = "ë¬¸ë§¥ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. target_kbë¥¼ md/csvë¡œ ë°”ê¾¸ê±°ë‚˜ ì§ˆë¬¸ì„ ë” êµ¬ì²´í™”í•´ ë³´ì„¸ìš”."
        chat_history.add_user_message(question)
        chat_history.add_ai_message(result)
        return {
            "mcp": {
                "source": "rag_agent",
                "destination": mcp["source"],
                "intent": "answer",
                "payload": {
                    "answer": result,
                    "references": [],
                    "metadata": {"session_id": session_id, "used_kb": target_kb, "top_k": top_k}
                }
            }
        }

    history_text = "\n".join([m.content for m in chat_history.messages])
    context = docs_to_context(docs)
    prompt = build_prompt(context=context, question=question, history_text=history_text)

    try:
        result = azure_llm.invoke(prompt).content
    except Exception as e:
        print("[LLM ERROR]", e.__class__.__name__, str(e))
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"LLM error: {e.__class__.__name__}: {e}")

    chat_history.add_user_message(question)
    chat_history.add_ai_message(result)

    refs = docs_to_refs(docs)
    return {
        "mcp": {
            "source": "rag_agent",
            "destination": mcp["source"],
            "intent": "answer",
            "payload": {
                "answer": result,
                "references": refs,
                "metadata": {"session_id": session_id, "used_kb": target_kb, "top_k": top_k}
            }
        }
    }

# LangGraph ì»´íŒŒì¼
rag_app = StateGraph(dict)
rag_app.set_entry_point("rag_node")
rag_app.add_node("rag_node", rag_node)
rag_app.set_finish_point("rag_node")
rag_app = rag_app.compile()

def handle_mcp(mcp: Dict[str, Any]) -> Dict[str, Any]:
    if mcp["destination"] == "rag_agent":
        return rag_app.invoke({"mcp": mcp})
    raise ValueError(f"Unknown destination: {mcp['destination']}")

# =========================================================
# API ìŠ¤í‚¤ë§ˆ & ì—”ë“œí¬ì¸íŠ¸
# =========================================================
class QARequest(BaseModel):
    question: str
    session_id: str
    target_kb: Optional[Literal["md", "csv", "all"]] = "all"
    k: int = 3

@app.get("/")
def health():
    return {"message": "RAG (MD + CSV) FastAPI ì„œë²„ ë™ì‘ ì¤‘"}

@app.post("/answer")
def answer_question(req: QARequest):
    print(f"âœ… /answer í˜¸ì¶œ: target_kb={req.target_kb}, k={req.k}")
    try:
        mcp = {
            "source": "user",
            "destination": "rag_agent",
            "intent": "get_answer",
            "payload": {
                "question": req.question,
                "metadata": {
                    "session_id": req.session_id,
                    "target_kb": req.target_kb or "all",
                    "k": req.k
                }
            }
        }
        response = handle_mcp(mcp)
        return response["mcp"]["payload"]
    except HTTPException:
        raise
    except Exception as e:
