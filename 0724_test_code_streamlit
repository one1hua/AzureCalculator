import os
import streamlit as st
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain_chroma import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI

# -------------------------------
# 1. Streamlit í˜ì´ì§€ ì„¤ì •
# -------------------------------
st.set_page_config(page_title="Azure VM Dì‹œë¦¬ì¦ˆ QA ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ’¬ Azure VM ê°€ê²© QA ì±—ë´‡ (Dì‹œë¦¬ì¦ˆ ì „ìš©)")

# -------------------------------
# 2. Azure ì„¤ì •ê°’
# -------------------------------
AZURE_ENDPOINT = "https://b2b1-md1hatmd-japaneast.openai.azure.com/"
AZURE_API_KEY = "4qbVhsgLzOqKX4RYpTl3VEUpLNw6AroXZJc4Mu3I9xYCZdnfswugJQQJ99BGACi0881XJ3w3AAAAACOG2pPY"
API_VERSION = "2024-12-01-preview"
EMBEDDING_DEPLOYMENT = "text-embedding-3-small"
CHAT_DEPLOYMENT = "gpt-4.1-mini"
CHROMA_DIR = "./chromaDB"

# -------------------------------
# 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

# -------------------------------
# 4. ìºì‹œëœ ë¦¬ì†ŒìŠ¤ ë¡œë”©
# -------------------------------
@st.cache_resource
def load_embedding_model():
    return AzureOpenAIEmbeddings(
        azure_endpoint=AZURE_ENDPOINT,
        azure_deployment=EMBEDDING_DEPLOYMENT,
        api_key=AZURE_API_KEY,
        openai_api_version=API_VERSION
    )

@st.cache_resource
def load_chat_model():
    return AzureChatOpenAI(
        api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT,
        azure_deployment=CHAT_DEPLOYMENT,
        openai_api_version=API_VERSION,
        temperature=0.3,
        max_tokens=800
    )
@st.cache_resource
def load_retriever(_embedding_model):  # ì–¸ë”ìŠ¤ì½”ì–´ ë¶™ì—¬ ìºì‹± ì˜ˆì™¸ ì²˜ë¦¬
    return Chroma(
        persist_directory=CHROMA_DIR,
        embedding_function=_embedding_model
    ).as_retriever(search_kwargs={"k": 3})

embedding_model = load_embedding_model()
llm = load_chat_model()
retriever = load_retriever(embedding_model)

# -------------------------------
# 5. RAG QA ì²´ì¸
# -------------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# -------------------------------
# 6. ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
# -------------------------------
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# -------------------------------
# 7. ì‚¬ìš©ì ì…ë ¥ ë° ì²˜ë¦¬
# -------------------------------
user_input = st.chat_input("Azure VM ê°€ê²©ì´ë‚˜ Dì‹œë¦¬ì¦ˆì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # QA ìˆ˜í–‰
    result = qa_chain.invoke({"query": user_input})
    answer = result["result"]
    source_docs = result["source_documents"]

    # ë‹µë³€ ì¶œë ¥ ë° ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": answer})
    with st.chat_message("assistant"):
        st.markdown(f"**ë‹µë³€:** {answer}")

        with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
            for i, doc in enumerate(source_docs, 1):
                st.markdown(f"{i}. {doc.page_content[:300]}...")
