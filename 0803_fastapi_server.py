from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
from langchain.schema import Document
from langchain_chroma import Chroma
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langgraph.graph import StateGraph
from langchain_core.chat_history import InMemoryChatMessageHistory

# FASTAPI ì„¤ì •
app = FastAPI()

# API Version ì„¤ì • (ìµœìƒë‹¨ì— ìœ„ì¹˜)
API_VERSION = "2024-12-01-preview"

# Azure OpenAI ì„¤ì •
AZURE_CHAT_ENDPOINT = "https://azure-openai-price01.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview"
AZURE_CHAT_DEPLOYMENT = "gpt-4o"
AZURE_EMBEDDING_ENDPOINT = "https://azure-openai-price01.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15"
AZURE_EMBEDDING_DEPLOYMENT = "text-embedding-3-small"
AZURE_API_KEY = "8D0TqUJQKj7SfHCMcRSAs5cJBSbvvQIcsuZ6QvHFKWsVsP9JyhooJQQJ99BGACNns7RXJ3w3AAABACOG0jZk"


# ëª¨ë¸ ì´ˆê¸°í™”
azure_llm = AzureChatOpenAI(
    api_key=AZURE_API_KEY,
    azure_endpoint=AZURE_CHAT_ENDPOINT,
    azure_deployment=AZURE_CHAT_DEPLOYMENT,
    openai_api_version=API_VERSION,
    temperature=0.3,
    max_tokens=800
)

embedding_model = AzureOpenAIEmbeddings(
    azure_endpoint=AZURE_EMBEDDING_ENDPOINT,
    azure_deployment=AZURE_EMBEDDING_DEPLOYMENT,
    api_key=AZURE_API_KEY,
    openai_api_version=API_VERSION
)


# Azure OpenAI ì„¤ì •
#AZURE_ENDPOINT = "https://azure-openai-price01.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview"
#AZURE_API_KEY = "8D0TqUJQKj7SfHCMcRSAs5cJBSbvvQIcsuZ6QvHFKWsVsP9JyhooJQQJ99BGACNns7RXJ3w3AAABACOG0jZk"
#API_VERSION = "2024-12-01-preview"
#EMBEDDING_DEPLOYMENT = "text-embedding-3-small"
#CHAT_DEPLOYMENT = "gpt-4o"

# ëª¨ë¸ ì´ˆê¸°í™”
#azure_llm = AzureChatOpenAI(
#    api_key=AZURE_API_KEY,
#    azure_endpoint=AZURE_ENDPOINT,
#    azure_deployment=CHAT_DEPLOYMENT,
#    openai_api_version=API_VERSION,
#    temperature=0.3,
#    max_tokens=800
#)

#embedding_model = AzureOpenAIEmbeddings(
#    azure_endpoint=AZURE_ENDPOINT,
#    azure_deployment=EMBEDDING_DEPLOYMENT,
#    api_key=AZURE_API_KEY,
#    openai_api_version=API_VERSION
#)

# Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë”©
retriever = Chroma(
    persist_directory="./chromaDB",
    embedding_function=embedding_model
).as_retriever(search_kwargs={"k": 3})

# ì„¸ì…˜ë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬
chats_by_session_id = {}
def get_chat_history(session_id: str):
    if session_id not in chats_by_session_id:
        chats_by_session_id[session_id] = InMemoryChatMessageHistory()
    return chats_by_session_id[session_id]

# RAG ë…¸ë“œ (Retrieval + LLM í˜¸ì¶œ)
def rag_node(state: Dict[str, Any]):
    mcp = state["mcp"]
    question = mcp["payload"]["question"]
    session_id = mcp["payload"]["metadata"]["session_id"]
    chat_history = get_chat_history(session_id)

    print(f"ğŸ” ì§ˆë¬¸: {question}")
    docs = retriever.invoke(question)
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    if not docs:
        raise ValueError("âŒ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    history_text = "\n".join([m.content for m in chat_history.messages])
    context = "\n".join(doc.page_content for doc in docs)

    full_context = f"{history_text}\n\n{context}" if history_text else context
    prompt = f"""ë‹¤ìŒì€ ë¬¸ë§¥ì…ë‹ˆë‹¤:\n{full_context}\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"""

    result = azure_llm.invoke(prompt).content

    chat_history.add_user_message(question)
    chat_history.add_ai_message(result)

    return {
        "mcp": {
            "source": "rag_agent",
            "destination": mcp["source"],
            "intent": "answer",
            "payload": {
                "answer": result,
                "references": [doc.page_content for doc in docs],
                "metadata": {"session_id": session_id}
            }
        }
    }

# LangGraph ì»´íŒŒì¼
rag_app = StateGraph(dict)
rag_app.set_entry_point("rag_node")
rag_app.add_node("rag_node", rag_node)
rag_app.set_finish_point("rag_node")
rag_app = rag_app.compile()

# MCP í•¸ë“¤ëŸ¬
def handle_mcp(mcp: Dict[str, Any]) -> Dict[str, Any]:
    if mcp["destination"] == "rag_agent":
        return rag_app.invoke({"mcp": mcp})
    else:
        raise ValueError(f"Unknown destination: {mcp['destination']}")

# Request Body
class QARequest(BaseModel):
    question: str
    session_id: str

# í—¬ìŠ¤ì²´í¬
@app.get("/")
def health():
    return {"message": "Azure RAG FastAPI ì„œë²„ ë™ì‘ ì¤‘"}

# ì§ˆë¬¸ ì—”ë“œí¬ì¸íŠ¸
@app.post("/answer")
def answer_question(req: QARequest):
    print(f"âœ… /answer API í˜¸ì¶œë¨: {req}")
    try:
        intent = "get_answer"
        mcp = {
            "source": "user",
            "destination": "rag_agent",
            "intent": intent,
            "payload": {
                "question": req.question,
                "metadata": {
                    "session_id": req.session_id
                }
            }
        }
        response = handle_mcp(mcp)
        return response["mcp"]["payload"]
    except Exception as e:
        print(f"âŒ Error while processing /answer: {e}")
        raise HTTPException(status_code=500, detail=str(e))

